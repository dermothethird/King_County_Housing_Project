{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiled Deliverable Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "pd.options.display.max_rows = 4000\n",
    "from numbers import Number\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Start: EDA and Transformation of Target Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the construction of our model is to set a target outcome for our model to predict. Due to the skew that was identified in our initial Exploratory Data Analysis for housing prices as a target outcome, a logarithmic transformation was performed to normalize the distribution and satisfy the linearity assumption needed for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data file\n",
    "df = pd.read_csv(\"./data/kc_house_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting outcome and saving as a variable for further use\n",
    "target = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize if price is normally distributed \n",
    "plt.subplots(figsize=(10,10))\n",
    "sns.histplot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if price is normally distributed \n",
    "target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed setting to display descriptive statistics without scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "target.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible outliers\n",
    "target.loc[target>= 6500000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle right-skew with log transform and reassess outlier impact on distribution\n",
    "ln_target = np.log(target)\n",
    "#visualize  \n",
    "plt.subplots(figsize=(10,10))\n",
    "sns.histplot(ln_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if price is normally distributed \n",
    "ln_target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skew is near zero - note the difference between median and mean in descriptive stats\n",
    "#check kurtosis\n",
    "stats.kurtosis(ln_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since fisher kurtosis score >=0, this distribution is very normal and we will accept this for our target outcome.\n",
    "#Note that due to this transformation, the final model will be interpreted in terms of logarithm of price rather \n",
    "#than simple dollar amounts\n",
    "\n",
    "#Save new target over old dataframe and rename new dataset\n",
    "df[\"price\"] = ln_target\n",
    "\n",
    "#rename\n",
    "final_df = df.rename(columns={\"price\":\"ln_price\"})\n",
    "\n",
    "#test\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new dataset as csv\n",
    "final_df.to_csv(\"./data/ln_price_dataframe\", index=False)\n",
    "#proceed to further eda in further notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Feature Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv('./data/kc_house_data.csv')\n",
    "final_df = pd.read_csv('./data/ln_price_dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Names and Descriptions for King County Data Set\n",
    "* `id` - Unique identifier for a house\n",
    "* `date` - Date house was sold\n",
    "* `price` - Sale price (prediction target)\n",
    "* `bedrooms` - Number of bedrooms\n",
    "* `bathrooms` - Number of bathrooms\n",
    "* `sqft_living` - Square footage of living space in the home\n",
    "* `sqft_lot` - Square footage of the lot\n",
    "* `floors` - Number of floors (levels) in house\n",
    "* `waterfront` - Whether the house is on a waterfront\n",
    "  * Includes Duwamish, Elliott Bay, Puget Sound, Lake Union, Ship Canal, Lake Washington, Lake Sammamish, other lake, and river/slough waterfronts\n",
    "* `view` - Quality of view from house\n",
    "  * Includes views of Mt. Rainier, Olympics, Cascades, Territorial, Seattle Skyline, Puget Sound, Lake Washington, Lake Sammamish, small lake / river / creek, and other\n",
    "* `condition` - How good the overall condition of the house is. Related to maintenance of house.\n",
    "  * See the [King County Assessor Website](https://info.kingcounty.gov/assessor/esales/Glossary.aspx?type=r) for further explanation of each condition code\n",
    "* `grade` - Overall grade of the house. Related to the construction and design of the house.\n",
    "  * See the [King County Assessor Website](https://info.kingcounty.gov/assessor/esales/Glossary.aspx?type=r) for further explanation of each building grade code\n",
    "* `sqft_above` - Square footage of house apart from basement\n",
    "* `sqft_basement` - Square footage of the basement\n",
    "* `yr_built` - Year when house was built\n",
    "* `yr_renovated` - Year when house was renovated\n",
    "* `zipcode` - ZIP Code used by the United States Postal Service\n",
    "* `lat` - Latitude coordinate\n",
    "* `long` - Longitude coordinate\n",
    "* `sqft_living15` - The square footage of interior housing living space for the nearest 15 neighbors\n",
    "* `sqft_lot15` - The square footage of the land lots of the nearest 15 neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sqft_living'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.corr()['price'].map(abs).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['zipcode'].hist(bins=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price is most strongly correlated with sqft_living and sqft_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicates = final_df[final_df.duplicated(['id','date'])].sort_values(by='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here you run your code to clean the data\n",
    "final_df.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['date'] = pd.to_datetime(final_df['date'])\n",
    "final_df['date'].sort_values().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This columns can help our stakeholder to understand when the sale prices were recorded, but won't help us much our model; therefore, we will omit it from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### waterfront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change YES/NO values to True/False\n",
    "final_df['waterfront'].replace({'YES': True, 'NO': False}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df.loc[final_df['waterfront'] == True].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the small amount of trues and large amount of nulls, we are going to assume this column was not recorded well and omit it from our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df['view'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df['view'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the 63 nulls and the majority of values being None, we are going to omit this column from our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to age and grade. Coded 1-5.\n",
    "\n",
    "1 = Poor- Worn out. Repair and overhaul needed on painted surfaces, roofing, plumbing, heating and numerous functional inadequacies. Excessive deferred maintenance and abuse, limited value-in-use, approaching abandonment or major reconstruction; reuse or change in occupancy is imminent. Effective age is near the end of the scale regardless of the actual chronological age.\n",
    "\n",
    "2 = Fair- Badly worn. Much repair needed. Many items need refinishing or overhauling, deferred maintenance obvious, inadequate building utility and systems all shortening the life expectancy and increasing the effective age.\n",
    "\n",
    "3 = Average- Some evidence of deferred maintenance and normal obsolescence with age in that a few minor repairs are needed, along with some refinishing. All major components still functional and contributing toward an extended life expectancy. Effective age and utility is standard for like properties of its class and usage.\n",
    "\n",
    "4 = Good- No obvious maintenance required but neither is everything new. Appearance and utility are above the standard and the overall effective age will be lower than the typical property.\n",
    "\n",
    "5= Very Good- All items well maintained, many having been overhauled and repaired as they have shown signs of wear, increasing the life expectancy and lowering the effective age with little deterioration or obsolescence evident with a high degree of utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ordinal Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the order of the categorical values matters (we expect a Very Good home to be higher in value than a Poor home), we will try to Ordinal Encode this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Ordinal Encoder\n",
    "condition = final_df['condition']\n",
    "condition_df = pd.DataFrame(condition)\n",
    "#cond_cat = [list(final_df['condition'].value_counts().keys())]\n",
    "cond_cat = [['Poor', 'Fair', 'Average', 'Good', 'Very Good']]\n",
    "ords_cond = OrdinalEncoder(categories=cond_cat)\n",
    "ords_cond.fit(condition_df)\n",
    "X_cond_transform = ords_cond.transform(condition_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the regression\n",
    "y = final_df['ln_price']\n",
    "X_condition_ord = sm.add_constant(X_cond_transform)\n",
    "X_condition_ord_results = sm.OLS(y, X_condition_ord).fit().summary()\n",
    "X_condition_ord_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a violin plot to understand correlation\n",
    "X_iterable = [i[0] for i in X_cond_transform]\n",
    "sns.violinplot(y=y, x=X_iterable);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See how correlated Condition is with Price\n",
    "cond_transform_df = pd.DataFrame(X_cond_transform)\n",
    "pd.concat([y, cond_transform_df], axis=1).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the very low rsquared value and low correlation value, these two variables aren't very correlated; however, the pvalue is lower than our alpha which means the predictor is statistically significant. To check, we're going to One Hot Encode these values to see if a specific categorial variable is very correlated to price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array into dataframe\n",
    "X_condition_ord_df = pd.DataFrame(X_cond_transform, columns=['cond_ord'])\n",
    "X_condition_ord_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cond = OneHotEncoder(drop='first') # drops average, first and most frequent\n",
    "ohe_cond.fit(condition_df)\n",
    "condition_hot = pd.DataFrame(ohe_cond.transform(condition_df).todense(), columns=ohe_cond.get_feature_names())\n",
    "#condition_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the regression\n",
    "X_cond_hot = sm.add_constant(condition_hot)\n",
    "X_condition_hot_results = sm.OLS(y, X_cond_hot).fit().summary()\n",
    "X_condition_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, this model has a higher rsquared meaning it accounts for more variance in the target variable (only slighlty). Given the results, we will most likely use the One Hot Encoded condition variable in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represents the construction quality of improvements. Grades run from grade 1 to 13. Generally defined as:\n",
    "\n",
    "1-3 Falls short of minimum building standards. Normally cabin or inferior structure.\n",
    "\n",
    "4 Generally older, low quality construction. Does not meet code.\n",
    "\n",
    "5 Low construction costs and workmanship. Small, simple design.\n",
    "\n",
    "6 Lowest grade currently meeting building code. Low quality materials and simple designs.\n",
    "\n",
    "7 Average grade of construction and design. Commonly seen in plats and older sub-divisions.\n",
    "\n",
    "8 Just above average in construction and design. Usually better materials in both the exterior and interior finish work.\n",
    "\n",
    "9 Better architectural design with extra interior and exterior design and quality.\n",
    "\n",
    "10 Homes of this quality generally have high quality features. Finish work is better and more design quality is seen in the floor plans. Generally have a larger square footage.\n",
    "\n",
    "11 Custom design and higher quality finish work with added amenities of solid woods, bathroom fixtures and more luxurious options.\n",
    "\n",
    "12 Custom design and excellent builders. All materials are of the highest quality and all conveniences are present.\n",
    "\n",
    "13 Generally custom designed and built. Mansion level. Large amount of highest quality cabinet work, wood trim, marble, entry ways etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df['grade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ordinal Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the order of the categorical values matters (we expect a Very Good home to be higher in value than a Poor home), we will try to Ordinal Encode this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Ordinal Encoder\n",
    "grade = final_df['grade']\n",
    "grade_df = pd.DataFrame(grade)\n",
    "grade_cat = [['3 Poor', '4 Low', '5 Fair', '6 Low Average', '7 Average', '8 Good', '9 Better', '10 Very Good', '11 Excellent', '12 Luxury', '13 Mansion']]\n",
    "ords_grade = OrdinalEncoder(categories=grade_cat)\n",
    "ords_grade.fit(grade_df)\n",
    "X_grade_transform = ords_grade.transform(grade_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the regression\n",
    "X_grade_ord = sm.add_constant(X_grade_transform)\n",
    "X_grade_ord_results = sm.OLS(y, X_grade_ord).fit().summary()\n",
    "X_grade_ord_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about regression above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array into dataframe\n",
    "X_grade_ord_df = pd.DataFrame(X_grade_transform, columns=['grade_ord'])\n",
    "X_grade_ord_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up One Hot Encoder\n",
    "ohe_grade = OneHotEncoder(drop='first') # drops average, first and most frequent\n",
    "ohe_grade.fit(grade_df)\n",
    "grade_hot = pd.DataFrame(ohe_grade.transform(grade_df).todense(), columns=ohe_grade.get_feature_names())\n",
    "#grade_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the regression\n",
    "X_grade_hot = sm.add_constant(grade_hot)\n",
    "X_grade_hot_results = sm.OLS(y, X_grade_hot).fit().summary()\n",
    "X_grade_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about regression above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqft_basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change sqft_basement data type to numeric\n",
    "#final_df['sqft_basement'].replace('?', 0, inplace=True)\n",
    "#final_df['sqft_basement'] = pd.to_numeric(df['sqft_basement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create CSV's for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use both condition and grade in our models, we are going to create csv's - combining the ordinal encoded data and ohe data into separate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concat Ordinal Encdoded Dataframes from condition and grade into one dataframe\n",
    "X_cat_ordinal_df = pd.concat([X_condition_ord_df, X_grade_ord_df], axis=1)\n",
    "X_cat_ordinal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ordinal dataframe into a csv\n",
    "X_cat_ordinal_df.to_csv(\"./data/cat_ordinal_dataframe\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concat Ordinal Encdoded Dataframes from condition and grade into one dataframe\n",
    "X_cat_hot_df = pd.concat([condition_hot, grade_hot], axis=1) \n",
    "X_cat_hot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note: From the condition table, \"Average\" value was dropped. From the grade table, \"10 Very Good\" was dropped. We defaulted to dropping the first value in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert ohe dataframe into a csv\n",
    "X_cat_hot_df.to_csv(\"./data/cat_hot_dataframe\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since sqft_living is the variable most correlated to price (0.70), we will start by creating a simple linear regression between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the simple linear regression between price and sqft_living\n",
    "simple_endog = final_df['ln_price']\n",
    "simple_exog_sqftliving = sm.add_constant(df['sqft_living'])\n",
    "\n",
    "simple_model = sm.OLS(simple_endog, simple_exog_sqftliving).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the simple linear regression model\n",
    "simple_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model only explains 49.3% of the target variable variance, which is quite low. The predicor variable however is significant given that it's pvalue is smaller than our .05 alpha. A one unit increase in sqft_living will increase the price of a home by $280.86. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's test out another highly correlated variable (0.61), sqft_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the simple linear regression between price and sqft_above\n",
    "simple_exog_bedroom = sm.add_constant(final_df['bedrooms'])\n",
    "\n",
    "simple_model_2 = sm.OLS(simple_endog, simple_exog_bedroom).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run the simple linear regression model\n",
    "simple_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this model explains less variance than the last at 36.6%. Sqft_above is also statistically significant, with a pvalue less than an alpha of .05. A one unit increase in sqft_above will increase the price of a home by $268.67."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv(\"./data/ln_price_dataframe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking import of data\n",
    "df                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical data being handled in separate notebook - keeping only numeric data\n",
    "df1 = df.drop(df[['date', 'view', 'waterfront','condition','grade','zipcode','lat', 'long' ]], axis = 1)\n",
    "df1.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking data types\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting non-numeric data types into numeric data types for later regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['sqft_basement'] = df1['sqft_basement'].replace('?', '0.0')\n",
    "df1['sqft_basement'] = pd.to_numeric(df1['sqft_basement'], errors='coerce')\n",
    "df1['sqft_basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['sqft_basement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data types \n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize distributions\n",
    "for a, column in enumerate(df1.columns):\n",
    "    plt.figure(a)\n",
    "    sns.distplot(df1[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the following: \n",
    "- severe outliers apparent in sqft_lot15, sqft_basement, sqft_lot\n",
    "- right tail skew in sqft_living15, sqft_above, sqft_above\n",
    "- bimodal distribution in yr_renovated with lots of null values\n",
    "- left tail skew in yr_built, possible multimodality due to spikes in housing construction\n",
    "\n",
    "*skewed distributions may require transformation to satisfy linear relationship assumption of linear regression. \n",
    "\n",
    "*floors, bedrooms, and bathrooms may be usable in their current state with minimal cleaning needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert yr_renovated into boolean data type indicating a house has been renovated or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in enumerate(df1[\"yr_renovated\"]):\n",
    "    if r == 0:\n",
    "        df1[\"yr_renovated\"][i] = False\n",
    "    else:\n",
    "        df1[\"yr_renovated\"][i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename yr_renovated\n",
    "df1.rename(columns={\"yr_renovated\": \"renovated\"}, inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode renovation data\n",
    "ohe_df = pd.DataFrame(df1[\"renovated\"])\n",
    "ohe = OneHotEncoder(drop='first') # drops average, first and most frequent\n",
    "ohe.fit(ohe_df)\n",
    "renovation_df = pd.DataFrame(ohe.transform(ohe_df).todense(), columns=ohe.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renovation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linreg on ohe, assumption check \n",
    "sm.OLS(df[\"ln_price\"], sm.add_constant(renovation_df)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"renovated\"] = renovation_df[\"x0_True\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Year of Construction to Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"yr_built\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1[\"yr_built\"] = 2015 - df1[\"yr_built\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns={\"yr_built\":\"age\"}, inplace=True)\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a closer look at sqft_lot15, sqft_basement, sqft_lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general look at central tendency of the 3 features with severe outliers\n",
    "df1[\"sqft_lot15\"].describe(), df1[\"sqft_basement\"].describe(), df1[\"sqft_lot\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "severe outliers present in sqft_lot15 - note the order of magnitude difference between 75th percentile and max\n",
    "\n",
    "large presence of nulls in sqft_basement\n",
    "\n",
    "sqft_lot similar to sqft_lot15 but with greater severity - max is two orders larger than 75th percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for Linearity Assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop basement sqft since majority of houses in king county do not have basements\n",
    "\n",
    "df1 = df1.drop(labels=\"sqft_basement\", axis=1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize scatter plot of each variable against the target outcome to check for linearity\n",
    "target = df1[\"ln_price\"]\n",
    "inputs = df1.drop(labels=[\"id\",\"ln_price\"], axis=1)\n",
    "for a, column in enumerate(inputs.columns):\n",
    "    plt.figure(a)\n",
    "    sns.scatterplot(y=target, x=inputs[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqft_above, sqft_living, possibly sqft_living15, are non-linear - need transform? or non-logarithmic target\n",
    "\n",
    "sqft_lot15 needs nulls addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no nulls found\n",
    "df1[\"sqft_lot15\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#natural log transform sqft_above - attempt to address non-linearity\n",
    "inputs[\"sqft_above\"] = np.log(inputs[\"sqft_above\"])\n",
    "sns.scatterplot(y=target, x=inputs[\"sqft_above\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#natural log transform sqft_living - attempt to address non-linearity\n",
    "inputs[\"sqft_living\"] = np.log(inputs[\"sqft_living\"])\n",
    "sns.scatterplot(y=target, x=inputs[\"sqft_living\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#natural log transform sqft_living15 - attempt to address non-linearity\n",
    "inputs[\"sqft_living15\"] = np.log(inputs[\"sqft_living15\"])\n",
    "sns.scatterplot(y=target, x=inputs[\"sqft_living15\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename all affected columns to reflect the changes above\n",
    "inputs.rename(columns={\"sqft_above\":\"ln_sqft_above\", \n",
    "                       \"sqft_living\":\"ln_sqft_living\",\n",
    "                       \"sqft_living15\":\"ln_sqft_living15\"},\n",
    "             inplace=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save initial model inputs and natural log target prices for use in final constructor notebook via csv\n",
    "inputs.to_csv(\"./data/initial_numeric_inputs\", index=False)\n",
    "target.to_csv(\"./data/house_price_target_natlog\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the construction for a multiple linear regression model predicting housing prices in King County. Data is imported from the preprocessed data that was cleaned in preceding notebooks and saved as separate csv files. The data is imported to this notebook and combined for the first model. Further iterations are contained in additional subsections of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error \n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Combine Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_ohe = pd.read_csv(\"./data/cat_hot_dataframe\")\n",
    "categorical_ordinal = pd.read_csv(\"./data/cat_ordinal_dataframe\")\n",
    "numeric = pd.read_csv(\"./data/initial_numeric_inputs\")\n",
    "target = pd.read_csv(\"./data/house_price_target_natlog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictors = pd.concat([numeric, categorical_ohe, categorical_ordinal], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining constructor function so it can be used for each iteration\n",
    "\n",
    "def construct_model(exog_df, endog=target):\n",
    "    ''' This function takes a dataframe of feature variables, performs robust scaling, \n",
    "    and returns summary statistics. '''\n",
    "   \n",
    "    rscale = RobustScaler()\n",
    "    rs = rscale.fit_transform(exog_df)\n",
    "    rs_df = pd.DataFrame(rs, columns=exog_df.columns, index=exog_df.index)\n",
    "    \n",
    "    exog = sm.add_constant(rs_df)\n",
    "    \n",
    "    model = sm.OLS(endog, exog).fit().summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#run 1st model\n",
    "\n",
    "#concatenate numeric and target to run a correlation\n",
    "run_corr = pd.concat([target, numeric], axis=1)\n",
    "run_corr.corr()['ln_price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ln_sqft_living is our highest correlated variable to ln_price, we will use it to run our first regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run first model\n",
    "construct_model(pd.DataFrame(all_predictors['ln_sqft_living']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accounts for roughly 45% of the target variable variance. ln_sqft_living is statistically signficant with a pvalue less than our alpha at .05. A one unit in iqr of log_sqft_living will increase our log_price by .4845."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Increase our rsquared with additional variables\n",
    "\n",
    "(2) Trim outliers as needed to imrove normality of inputs (JB Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that concatenates our next feature variable to our exog dataframe\n",
    "current_model = pd.DataFrame(all_predictors['ln_sqft_living'])\n",
    "\n",
    "def iterate(feature):\n",
    "    feature_df = pd.DataFrame(feature)\n",
    "    model = pd.concat([current_model, feature_df], axis=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run 2nd model adding ln_sqft_living15\n",
    "current_model = iterate(all_predictors['ln_sqft_living15'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 value was increased, however, some multicollinearity was observed and there are still significant deviations from normality in our inputs overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 3rd model\n",
    "current_model = iterate(all_predictors['ln_sqft_above'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 indicates our model's correlation has improved marginally, however, normality has also been significantly improved. Feature will be retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run 4th model\n",
    "# add bathrooms\n",
    "current_model = iterate(all_predictors['bathrooms'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 value indicates the model has increased correlation strength at the cost of slightly increased collinearity and decreased normality in our inputs. Feature retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 4th model\n",
    "# add bedrooms\n",
    "current_model = iterate(all_predictors['bedrooms'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 and JB have both been significantly improved by the new feature, albeit at a slight cost to collinearity. Feature retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 6th model\n",
    "# add floors\n",
    "current_model = iterate(all_predictors['floors'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New feature increased R2 value, although it caused increased skew/JB significance and collinearity. Feature retained due to additional increase in kurtosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 7th model\n",
    "# add sqft_lot\n",
    "current_model = iterate(all_predictors['sqft_lot'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no change in rqsquared, dropping from exog dataframe\n",
    "current_model.drop(labels='sqft_lot', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature dropped from successive models due to an unacceptable increase in collinearity for a negligible increase in correlation strength or other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 8th model\n",
    "# add sqft_lot15\n",
    "current_model = iterate(all_predictors['sqft_lot15'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no change in rqsquared, dropping from exog dataframe\n",
    "current_model.drop(labels='sqft_lot15', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature dropped from successive models due to an unacceptable increase in collinearity for a negligible increase in correlation strength or other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 9th model\n",
    "# add renovated\n",
    "current_model = iterate(all_predictors['renovated'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no change in rqsquared, dropping from exog dataframe\n",
    "current_model.drop(labels='renovated', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature dropped from successive models due to an unacceptable increase in collinearity for a negligible increase in correlation strength or other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 10th model\n",
    "# add conditional ordinal encoded\n",
    "current_model = iterate(categorical_ordinal['cond_ord'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant increase in the model's correlation strength and slight increase in skew. Feature retained due to increased kurtosis and correlation strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run 11th model\n",
    "# add grade ordinal encoded\n",
    "current_model = iterate(categorical_ordinal['grade_ord'])\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "signficant improvement in all metrics except for collinearity. Model is still within limits of collinearity assumption (Cond. No. <10), so feature is retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run 12th model\n",
    "# removes condition of house variable to assess impact on collinearity\n",
    "current_model.drop(labels='cond_ord', axis=1, inplace=True)\n",
    "construct_model(current_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model suffered penalties to correlation strength without improving collinearity. Feature will be retained in possible final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run 13th model\n",
    "# add grade ordinal encoded\n",
    "# assessing whether one-hot-encoding offers performance benefits compared to ordinal encoding for \n",
    "# categorical variables/features.\n",
    "current_model.drop(labels='grade_ord', axis=1, inplace=True)\n",
    "ohe_df = pd.concat([current_model, categorical_ohe], axis=1)\n",
    "construct_model(ohe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing ordinal encoding with one-hot-encoding results in mild improvement to correlation strength, but also results in unacceptable collinearity increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14th Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run 13th model\n",
    "# add grade ordinal encoded\n",
    "ohe_df.drop(labels=['x0_Fair', 'x0_Good', 'x0_Poor', 'x0_Very Good'], axis=1, inplace=True)\n",
    "construct_model(ohe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Potential Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar trade-offs observed with 13th and 14th models. Due to collinearity increase, ordinal encoding is preferred for the final model despite correlation strength improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12th iteration was the best fit we could produce with our available features without violating assumptions\n",
    "#here we recreate that dataset for use in the final model\n",
    "semi_final_df = pd.concat([target, numeric, categorical_ordinal], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_final_df.drop(labels=['sqft_lot', 'age', 'renovated', 'sqft_lot15'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#QQ plot generated for each feature to identify outliers that may need to be dropped before \n",
    "#the final model is created\n",
    "for e, f in enumerate(semi_final_df.columns):\n",
    "    sm.qqplot(semi_final_df[f], line='r');\n",
    "    plt.title(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QQ plot indicates that bedrooms are significantly affected by outliers, and bathrooms may be significantly skewed/non-normally distributed. Final iteration of our model will experiment with removal of bathrooms as a feature and trimming outliers from the dataset that are affecting bedroom regression modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional feature cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify houses with the most bedrooms (top 10)\n",
    "bed_10 = semi_final_df.sort_values(by=\"bedrooms\", ascending=False)[0:10]\n",
    "bed_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the house with 33 bedrooms has a smaller number of bathrooms than houses with 1/3 the number of \n",
    "#bedrooms, it is likely that this data is actually a typo and is significantly skewing the data.\n",
    "#Therefore, we will drop this house from the overall dataset before constructing the final model.\n",
    "#index position is 15856, however, we will reevaluate to ensure we dropped the correct data point\n",
    "\n",
    "semi_final_df.drop(index=15856, inplace=True)\n",
    "#reevaluate\n",
    "bed_10 = semi_final_df.sort_values(by=\"bedrooms\", ascending=False)[0:10]\n",
    "bed_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimenting with dropping the bathrooms feature due to non-normality and heavy presence of outliers\n",
    "new_target = semi_final_df[\"ln_price\"]\n",
    "final_inputs = semi_final_df.drop(columns=[\"bathrooms\",\"ln_price\"])\n",
    "construct_model(final_inputs, new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final comparison model with bathroom data retained\n",
    "new_target = semi_final_df[\"ln_price\"]\n",
    "final_inputs = semi_final_df.drop(columns=[\"ln_price\"])\n",
    "construct_model(final_inputs, new_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Final Final Model\n",
    "final_target_df = semi_final_df[\"ln_price\"]\n",
    "final_inputs_df = semi_final_df.drop(columns=[\"bathrooms\",\"ln_price\"])\n",
    "\n",
    "#construct final model and prep data for export\n",
    "#scale the data\n",
    "rscale = RobustScaler()\n",
    "rs = rscale.fit_transform(final_inputs_df)\n",
    "rs_df = pd.DataFrame(rs, columns=final_inputs_df.columns, index=final_target_df.index)\n",
    "\n",
    "#final construction    \n",
    "exog = sm.add_constant(rs_df)\n",
    "endog = final_target_df\n",
    "final_model = sm.OLS(endog, exog).fit()\n",
    "\n",
    "#final dataframe prepping for export (1) rename scaled data (2) join all 3 dataframes\n",
    "#1\n",
    "rs_df.rename(columns={name:f'scl_{name}' for name in rs_df.columns}, inplace=True)\n",
    "#2\n",
    "final_df = pd.concat([final_target_df, final_inputs_df, rs_df], axis=1)\n",
    "\n",
    "#generating summary statistics\n",
    "fmss = final_model.summary()\n",
    "fmss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = final_model.predict(exog)\n",
    "prediction # scatter endog and pred \n",
    "#polyfit to draw line (predicted, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract coefficients table\n",
    "accursed_table = pd.DataFrame(fmss.tables[1].data[0:])\n",
    "accursed_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace columns with the appropriate headers\n",
    "new_header = accursed_table.iloc[0]\n",
    "accursed_table = accursed_table[1:]\n",
    "accursed_table.columns = new_header\n",
    "accursed_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort table by coefficients\n",
    "blursed_table = accursed_table.sort_values(by=\"coef\", ascending=False)\n",
    "blursed_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top two features increasing home sale price in 2014-2015 were ln_sqft_living, and grade_ord \n",
    "#from the assessors office for King County, Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations and Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize model for living space correlation\n",
    "\n",
    "#variable declaration: see model summary for coefficient and intercept values\n",
    "x = final_df[\"ln_sqft_living\"]\n",
    "y = final_df[\"ln_price\"]\n",
    "m = 0.3188\n",
    "b0 = 12.8938\n",
    "y_ticks_liv = [11, 12, 13, 14, 15, 16]\n",
    "x_ticks_liv = [6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "fig = plt.scatter(x=x,y=y, color=\"#34b456\")\n",
    "\n",
    "ax.set_xlabel('Sqft Living Space (Log unit)', fontsize=24, labelpad=20) \n",
    "ax.set_ylabel('House Unit Sale Price (Log USD)', fontsize=24, labelpad=25) \n",
    "ax.set_title('Living Space vs. House Sale Price', fontsize=30, pad=15); \n",
    "\n",
    "ax.set_ylim(11, 16, 1)\n",
    "ax.set_yticks(y_ticks_liv)\n",
    "ax.set_yticklabels(y_ticks_liv, fontsize=17)\n",
    "ax.set_xlim(6, 9.5, 1)\n",
    "ax.set_xticks(x_ticks_liv)\n",
    "ax.set_xticklabels(x_ticks_liv, fontsize=17)\n",
    "\n",
    "plt.savefig('./Results/Sqft_Living_v_Sale_Price.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the raw correlation of living space v house sales (non-transformed)\n",
    "#variable declaration: see model summary for coefficient and intercept values\n",
    "x = np.exp(final_df[\"ln_sqft_living\"])\n",
    "y = np.exp(final_df[\"ln_price\"])\n",
    "m = 0.3188\n",
    "b0 = 12.8938\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "fig = plt.scatter(x=x,y=y, color=\"green\")\n",
    "\n",
    "ax.set_xlabel('Sqft Living Space', fontsize=24, labelpad=20) \n",
    "ax.set_ylabel('House Sale Price (USD)', fontsize=24, labelpad=25) \n",
    "ax.set_title('Living Space vs. House Sale Price (Raw)', fontsize=30, pad=15); \n",
    "\n",
    "x_val=[f\"{x*2},000\" for x in range(-1,8)]\n",
    "y_val=[f\"${y},000,000\" for y in range(0,9)]\n",
    "ax.set_xticks=[x*2000 for x in range(-1,8)]\n",
    "ax.set_yticks=[y*1000000 for y in range(0,9)]\n",
    "ax.set_xticklabels(x_val, fontsize=17)\n",
    "ax.set_yticklabels(y_val, fontsize=17)\n",
    "\n",
    "plt.savefig('./Results/Raw_Sqft_Living_v_Sale_Price.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#visualize model overall performance\n",
    "\n",
    "# Change font family\n",
    "plt.rcParams.update({'font.family':'Gill Sans'})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "fig = plt.scatter(x=prediction,y=endog, color=\"red\")\n",
    "y_ticks = [11, 11.5, 12, 12.5, 13, 13.5, 14, 14.5, 15, 15.5, 16]\n",
    "x_ticks = [11.5, 12, 12.5, 13, 13.5, 14, 14.5, 15]\n",
    "\n",
    "ax.set_xlabel('Predicted Unit House Sales (Log USD)', fontsize=24, labelpad=20) #color='white\n",
    "ax.set_ylabel('Actual Unit House Sales (Log USD)', fontsize=24, labelpad=25) #color='white'\n",
    "ax.set_title('Actual vs Predicted House Sales', fontsize=30, pad=15) #color='white'\n",
    "\n",
    "ax.set_ylim(11, 16, .5)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(y_ticks, fontsize=17)\n",
    "ax.set_xlim(11.5, 15, .5)\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_ticks, fontsize=17, rotation=45)\n",
    "\n",
    "#ax.set_xticklabels(prediction, fontsize=15, rotation=45); #color='white'\n",
    "\n",
    "m, b0 = np.polyfit(prediction, endog, deg=1)\n",
    "plt.plot(prediction, m*prediction+b0);\n",
    "plt.savefig('./Results/Actual_v_Predicted.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#visualize model for living space correlation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "fig = plt.scatter(x=np.exp(prediction),y=np.exp(endog), color=\"red\")\n",
    "\n",
    "ax.set_xlabel('Predicted House Sales (Millions USD)', fontsize=24, labelpad=20) #color='white\n",
    "ax.set_ylabel('Actual House Sales (Millions USD)', fontsize=24, labelpad=25) #color='white'\n",
    "ax.set_title('Actual vs Predicted House Sales (Raw)', fontsize=30, pad=15) #color='white'\n",
    "\n",
    "x_val=[f\"${x/2}\" for x in range(-1,7)]\n",
    "y_val=[f\"${y}\" for y in range(0,9)]\n",
    "ax.set_xticks=[(x/2)*1000000 for x in range(-1,7)]\n",
    "ax.set_yticks=[y*1000000 for y in range(0,9)]\n",
    "ax.set_xticklabels(x_val, fontsize=17, rotation=45)\n",
    "ax.set_yticklabels(y_val, fontsize=17)\n",
    "\n",
    "m, b0 = np.polyfit(np.exp(prediction), np.exp(endog), deg=1)\n",
    "plt.plot(np.exp(prediction), m*(np.exp(prediction))+b0);\n",
    "plt.savefig(\"./Results/Raw_Actual_v_Predicted.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'coef': {'Sqft Above': -0.1164, 'Bedrooms': -0.0303, \n",
    "                        'Sqft Living': 0.3188, 'Grade': 0.2023, 'Sqft Living 15': 0.1017, \n",
    "                        'Condition': 0.1010, 'Floors': 0.0654}})\n",
    "coef_sorted_df = coef_df.sort_values(by='coef', ascending=False).reset_index()\n",
    "coef_sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#vizualize model for coefficients\n",
    "\n",
    "# Create variables for chart\n",
    "coef_x = coef_sorted_df['index']\n",
    "coef_y = coef_sorted_df['coef']\n",
    "y_ticks = [-.2, -.1, 0, .1, .2, .3, .4]\n",
    "c = ['#34b456', '#34b456', '#747474', '#747474', '#747474', '#747474', '#747474']\n",
    "# Change font family\n",
    "plt.rcParams.update({'font.family':'Gill Sans'})\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "\n",
    "bars = ax.bar(coef_x, coef_y, color=c)\n",
    "ax.set_xlabel('Feature Variabes', fontsize=27, labelpad=20) #color='white'\n",
    "ax.set_ylabel('Coefficient', fontsize=27, labelpad=25) #color='white'\n",
    "ax.set_title('Feature Variables Effect on House Sale Price', fontsize=30, pad=15) #color='white'\n",
    "ax.set_ylim(-.2, .4, 1)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(y_ticks, fontsize=20) #color='white'\n",
    "ax.set_xticks(coef_x)\n",
    "ax.set_xticklabels(coef_x, fontsize=20, rotation=45); #color='white'\n",
    "plt.savefig(\"./Results/Coeff_Bar_Chart.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(final_df.groupby(by='grade_ord', axis=0).mean()['ln_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_ord_viz_df = pd.DataFrame({'Grade': {'Poor': 12.1112, 'Low': 12.1602, \n",
    "                        'Fair': 12.3233, 'Low Average': 12.5438, 'Average': 12.8365, \n",
    "                        'Good': 13.1362, 'Better': 13.4874, 'Very Good': 13.8030, 'Excellent': 14.1319, 'Luxury': 14.5147,\n",
    "                        'Mansion': 15.0282}}).reset_index()\n",
    "grade_ord_sort_viz_df = grade_ord_viz_df.sort_values(by='Grade', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_ord_sort_viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize model for assessor grade correlation\n",
    "#vizualize model for coefficients\n",
    "\n",
    "# Create variables for chart\n",
    "grade_ord_x = grade_ord_sort_viz_df['index']\n",
    "ln_price_y = grade_ord_sort_viz_df['Grade']\n",
    "y_ticks = [12, 12.5, 13, 13.5, 14, 14.5, 15, 15.5]\n",
    "c = ['#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456']\n",
    "# Change font family\n",
    "plt.rcParams.update({'font.family':'Gill Sans'})\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize = (15,10))\n",
    "\n",
    "bars = ax.bar(grade_ord_x, ln_price_y, color=c)\n",
    "ax.set_xlabel('Grade Values', fontsize=27, labelpad=20, color='white') #color='white'\n",
    "ax.set_ylabel('Average Unit Sale Price (Log USD)', fontsize=27, labelpad=25, color='white') #color='white'\n",
    "ax.set_title('House Grade vs Average Sale Price', fontsize=30, pad=15, color='white') #color='white'\n",
    "ax.set_ylim(12, 13, .5)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(y_ticks, fontsize=20, color='white') #color='white'\n",
    "ax.set_xticks(grade_ord_x)\n",
    "ax.set_xticklabels(grade_ord_x, fontsize=20, rotation=45, color='white'); #color='white'\n",
    "\n",
    "plt.savefig('./Results/Grade_vs_Price.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize model for assessor grade correlation (raw price)\n",
    "#vizualize model for coefficients\n",
    "\n",
    "# Create variables for chart\n",
    "grade_ord_x = grade_ord_sort_viz_df['index']\n",
    "ln_price_y = np.exp(grade_ord_sort_viz_df['Grade'])\n",
    "y_ticks = [y/2 for y in range(0,8)]\n",
    "c = ['#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456', '#34b456']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize = (15,10))\n",
    "\n",
    "bars = ax.bar(grade_ord_x, ln_price_y, color=c)\n",
    "ax.set_xlabel('Grade Values', fontsize=27, labelpad=20, color='white') #color='white'\n",
    "ax.set_ylabel('Average Sale Price (Millions USD)', fontsize=27, labelpad=25, color='white') #color='white'\n",
    "ax.set_title('House Grade vs Average Sale Price (Raw)', fontsize=30, pad=15, color='white') #color='white'\n",
    "\n",
    "ax.set_yticks([y*1000000 for y in y_ticks])\n",
    "ax.set_yticklabels(y_ticks, fontsize=20, color='white') #color='white'\n",
    "ax.set_xticks(grade_ord_x)\n",
    "ax.set_xticklabels(grade_ord_x, fontsize=20, rotation=45, color='white'); #color='white'\n",
    "\n",
    "plt.savefig('./Results/Raw_Grade_vs_Price.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.groupby(by='grade_ord', axis=0).mean()['ln_price']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
